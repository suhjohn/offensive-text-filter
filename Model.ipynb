{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import gensim \n",
    "import pandas\n",
    "\n",
    "FILENAME = './hate-speech-and-offensive-language/data/labeled_data.csv'\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nights'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import spell \n",
    "maxlen = 36\n",
    "kernel_size = 3\n",
    "filter_num = [32, 64, 128]\n",
    "# Following code is to check how the api works\n",
    "# <word> in word2vec_model.vocab checks if word is in vocab\n",
    "# \"nigggas\" in word2vec_model.vocab \n",
    "# # get the 10 closest word\n",
    "spell('niggas')\n",
    "\n",
    "# # get the vector for \"good\"\n",
    "# word2vec_model[\"gooood\"]\n",
    "\n",
    "# # size of the array (300)\n",
    "# len(word2vec_model[\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# String cleaning example\n",
    "example_list = [\"good \\n\"]\n",
    "[word.replace(\" \", \"\").replace(\"\\n\", \"\") for word in example_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larry, larry 0.0\n",
      "Celtics, celtic 0.0\n",
      "Bird, birds 0.0\n",
      "$TWTR. $TWTR. 0.0\n",
      "bro?? brown 0.0\n",
      "@CheezMoeJenkinz @CheezMoeJenkinz 0.0\n",
      "2-3:10am 2-3:10am 0.0\n",
      "assoc's hassocks 0.0\n",
      "tech! tech 0.0\n",
      "http://t.co/XfSeZSzu0m http://t.co/XfSeZSzu0m 0.0\n",
      "@BeforeItsNews @BeforeItsNews 0.0\n",
      "pollito pollitt 0.0\n",
      "cruzaba cruzado 0.0\n",
      "frontera, frontera 0.0\n",
      "zorro zorro 9.317334241932162e-08\n",
      "tropieza tropez 0.0\n",
      "pollo, pollok 0.0\n",
      "zorro zorro 9.317334241932162e-08\n",
      "dice, dice 0.0\n",
      "im... i.e. 0.0\n",
      "http://t.co/oXcOYauJi1 http://t.co/oXcOYauJi1 0.0\n",
      "&#8220;@NoBeeetch: &#8220;@NoBeeetch: 0.0\n",
      "\"@Trelaire1st: \"@Trelaire1st: 0.0\n",
      "16. 16. 0.0\n",
      "best? best 0.0\n",
      "http://t.co/KmOrdNEZRZ\"&#8221; http://t.co/KmOrdNEZRZ\"&#8221; 0.0\n",
      "of of 0.030455114922485163\n",
      "regulations. regulations 0.0\n",
      "of of 0.030455114922485163\n",
      "trash, trash 0.0\n",
      "kids. kids 0.0\n",
      "http://t.co/aaMJhHOW7y http://t.co/aaMJhHOW7y 0.0\n",
      "a a 0.02226710370623679\n",
      "to to 0.026506200913695065\n",
      "denton denton 1.0973749218275656e-06\n",
      "worm. worms 0.0\n",
      "to to 0.026506200913695065\n",
      "a a 0.02226710370623679\n",
      "birdddd birdied 0.0\n",
      "http://t.co/7gebzU4Paz http://t.co/7gebzU4Paz 0.0\n",
      "http://t.co/WwL05Orih6 http://t.co/WwL05Orih6 0.0\n",
      "Video: video 0.0\n",
      "http://t.co/qHrA6OalRR http://t.co/qHrA6OalRR 0.0\n",
      "Video: video 0.0\n",
      "http://t.co/9ArtP5ymbS http://t.co/9ArtP5ymbS 0.0\n",
      "Video: video 0.0\n",
      "http://t.co/vdaOWLZksv http://t.co/vdaOWLZksv 0.0\n",
      "Video: video 0.0\n",
      "Record: record 0.0\n",
      "http://t.co/lXK0YyCVR4 http://t.co/lXK0YyCVR4 0.0\n",
      "Video: video 0.0\n",
      "Glizzy lizzy 0.0\n",
      "&#8211; &#8211; 0.0\n",
      "&#8220;I &#8220;I 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Functions for model\n",
    "\"\"\"\n",
    "import pandas\n",
    "# from autocorrect import spell \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# for the four data sets \n",
    "# loop 0 to 35 and if in the sentence, replace it with the word's value from wordtovec\n",
    "# if no word then put in 0 instead\n",
    "TRAINING_OFFENSIVE_FILENAME = \"./training_offensive.csv\"\n",
    "TRAINING_REGULAR_FILENAME = \"./training_regular.csv\"\n",
    "TEST_OFFENSIVE_FILENAME = \"./test_offensive.csv\"\n",
    "TEST_REGULAR_FILENAME = \"./test_regular.csv\"\n",
    "\n",
    "off_train_dataframe = pandas.read_csv(TRAINING_OFFENSIVE_FILENAME)\n",
    "off_test_dataframe = pandas.read_csv(TEST_OFFENSIVE_FILENAME)\n",
    "reg_train_dataframe = pandas.read_csv(TRAINING_REGULAR_FILENAME)\n",
    "reg_test_dataframe = pandas.read_csv(TEST_REGULAR_FILENAME)\n",
    "\n",
    "\n",
    "# [[\"<word vector>\", \"<word vector>\"....\"<word vector>\"]    ]\n",
    "# The inside array is the tweet in its vectorized form\n",
    "# the inside array is always length of 36; we pad it with blank string if tweet is less than 36\n",
    "# dataframe is data\n",
    "def tokenize(dataframe):\n",
    "    max_word = 36\n",
    "    rows = list(dataframe.iterrows())\n",
    "    rows = [_[1] for _ in rows] # iterrows returns [(index, row), (index, row),...] so we pull out row\n",
    "    tokenized_values = []\n",
    "    for index, row in enumerate(rows[0:training_row_count]):\n",
    "        # Clean tweets to be processable\n",
    "        tweet = row.get(\"tweet\")\n",
    "        words = tweet.split(\" \") # assume a word is delinated by a space\n",
    "        words = [word for word in words if word != \"\"] # remove empty words\n",
    "        # jump to next row if for some reason empty tweet (just in case)\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        # clean words to be real words\n",
    "        words = [word.replace(\" \", \"\").replace(\"\\n\", \"\") for word in words]\n",
    "        # Replace words with the closest actual word\n",
    "        replaced_words = []\n",
    "        for word in words:\n",
    "            if word in word2vec_model:\n",
    "                replaced_words.append(word)\n",
    "            else:\n",
    "                most_similar_word = word2vec_model.similar_by_word(word)[0]\n",
    "                replaced_words.append(most_similar_word)\n",
    "        \n",
    "        \n",
    "        tweet_vectors = []\n",
    "        # convert the words to vectorized form\n",
    "        \n",
    "        # extract the dimension of a word\n",
    "        \n",
    "        # create a fake dummy vector that fits the dimension of the word\n",
    "        \n",
    "        # add it to the word vector list which should be of length 36\n",
    "        # Pad extra vectors to make it fit 36\n",
    "        while len(words) < max_word:\n",
    "            words.append(0)    \n",
    "        # Check if length is 36\n",
    "        assert len(tweet_vectors) == max_word\n",
    "        # Check if all the vector dimensions are equal\n",
    "        \n",
    "        # put\n",
    "        tokenized_values.append(tweet_vectors)\n",
    "    return tokenized_values\n",
    "    \n",
    "def test(dataframe):\n",
    "    spell = SpellChecker()\n",
    "    max_word = 36\n",
    "    rows = list(dataframe.iterrows())\n",
    "    rows = [_[1] for _ in rows] # iterrows returns [(index, row), (index, row),...] so we pull out row\n",
    "    tokenized_values = []\n",
    "    for index, row in enumerate(rows):\n",
    "        # Clean tweets to be processable\n",
    "        tweet = row.get(\"tweet\")\n",
    "        words = tweet.split(\" \") # assume a word is delinated by a space\n",
    "        words = [word for word in words if word != \"\"] # remove empty words\n",
    "        # jump to next row if for some reason empty tweet (just in case)\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        # clean words to be real words\n",
    "        words = [word.replace(\" \", \"\").replace(\"\\n\", \"\") for word in words]\n",
    "        for word in words:\n",
    "            if word not in word2vec_model:\n",
    "                modified_word = spell.correction(word)\n",
    "                probability = spell.word_probability(word)\n",
    "                print(word, modified_word, probability)\n",
    "\n",
    "test(reg_test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad with zeros\n",
    "off_train = pad_sequences(off_train, padding = 'post', maxlen = maxlen)\n",
    "off_test = pad_sequences(off_test, padding = 'post', maxlen = maxlen)\n",
    "reg_train = pad_sequences(reg_train, padding = 'post', maxlen = maxlen)\n",
    "reg_test = pad_sequences(reg_test, padding = 'post', maxlen = maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/drew/.pyenv/versions/3.7.1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv1D(filter_num, kernel_size, activation = 'relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(10, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Off_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3fc65384aaaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(Off_train, On_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOff_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     batch_size=10)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Off_train' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(off_train, reg_train,\n",
    "                    epochs=10,\n",
    "                    verbose=False,\n",
    "                    validation_data=(off_test, reg_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(Off_train, On_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(Off_test, On_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
