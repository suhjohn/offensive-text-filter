{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/drew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/drew/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this prior to running test\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the four data sets \n",
    "# loop 0 to 35 and if in the sentence, replace it with the word's value from wordtovec\n",
    "# if no word then put in 0 instead\n",
    "TRAINING_OFFENSIVE_FILENAME = \"./training_offensive.csv\"\n",
    "TRAINING_REGULAR_FILENAME = \"./training_regular.csv\"\n",
    "TEST_OFFENSIVE_FILENAME = \"./test_offensive.csv\"\n",
    "TEST_REGULAR_FILENAME = \"./test_regular.csv\"\n",
    "LOCAL_TRAINING_OFFENSIVE_FILENAME = \"./training_offensive_local.csv\"\n",
    "LOCAL_TRAINING_REGULAR_FILENAME = \"./training_regular_local.csv\"\n",
    "LOCAL_TEST_OFFENSIVE_FILENAME = \"./test_offensive_local.csv\"\n",
    "LOCAL_TEST_REGULAR_FILENAME = \"./test_regular_local.csv\"\n",
    "\n",
    "#FB_MODEL_FILENAME = \"./cc.en.300.bin\"\n",
    "#with open(LOCAL_TEST_REGULAR_FILENAME) as f:\n",
    "#    print(f.read())\n",
    "# fb_model = load_facebook_model(FB_MODEL_FILENAME) # do this in real code\n",
    "# off_train_dataframe = pandas.read_csv(TRAINING_OFFENSIVE_FILENAME)\n",
    "# off_test_dataframe = pandas.read_csv(TEST_OFFENSIVE_FILENAME)\n",
    "# reg_train_dataframe = pandas.read_csv(TRAINING_REGULAR_FILENAME)\n",
    "# reg_test_dataframe = pandas.read_csv(TEST_REGULAR_FILENAME)\n",
    "off_train_dataframe = pandas.read_csv(LOCAL_TRAINING_OFFENSIVE_FILENAME)\n",
    "off_test_dataframe = pandas.read_csv(LOCAL_TEST_OFFENSIVE_FILENAME)\n",
    "reg_train_dataframe = pandas.read_csv(LOCAL_TRAINING_REGULAR_FILENAME)\n",
    "reg_test_dataframe = pandas.read_csv(LOCAL_TEST_REGULAR_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "non_alphabet_pattern = re.compile(\"[^a-zA-Z]\")\n",
    "#NULL_VECTOR = fb_model.wv[\"\"]\n",
    "\n",
    "def get_cleaned_tokens(sentence):    \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = [re.sub(non_alphabet_pattern, \"\", w) for w in filtered_sentence]\n",
    "    filtered_sentence = [w for w in filtered_sentence if w != \"\"]\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
      "1     !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
      "2     !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
      "3     !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
      "4     !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
      "5     !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
      "6     !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
      "7     \" &amp; you might not get ya bitch back &amp; ...\n",
      "8     \" @rhythmixx_ :hobbies include: fighting Maria...\n",
      "9     \" Keeks is a bitch she curves everyone \" lol I...\n",
      "10                                                  NaN\n",
      "11                                                  NaN\n",
      "12                                                  NaN\n",
      "13                                                  NaN\n",
      "14                                                  NaN\n",
      "15                                                  NaN\n",
      "16                                                  NaN\n",
      "17                                                  NaN\n",
      "18                                                  NaN\n",
      "19                                                  NaN\n",
      "20                                                  NaN\n",
      "21                                                  NaN\n",
      "22                                                  NaN\n",
      "23                                                  NaN\n",
      "24                                                  NaN\n",
      "25                                                  NaN\n",
      "26                                                  NaN\n",
      "27                                                  NaN\n",
      "28                                                  NaN\n",
      "29                                                  NaN\n",
      "30                                                  NaN\n",
      "31                                                  NaN\n",
      "32                                                  NaN\n",
      "33                                                  NaN\n",
      "34                                                  NaN\n",
      "35                                                  NaN\n",
      "36                                                  NaN\n",
      "37                                                  NaN\n",
      "38                                                  NaN\n",
      "39                                                  NaN\n",
      "40                                                  NaN\n",
      "41                                                  NaN\n",
      "42                                                  NaN\n",
      "43                                                  NaN\n",
      "44                                                  NaN\n",
      "45                                                  NaN\n",
      "46                                                  NaN\n",
      "47                                                  NaN\n",
      "48                                                  NaN\n",
      "49                                                  NaN\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create Training Tweets\n",
    "offensive_train_tweets = off_train_dataframe['tweet']\n",
    "non_offensive_train_tweets = reg_train_dataframe['tweet']\n",
    "# offensive_train_tweets = [get_cleaned_tokens(tweet) for tweet in off_train_dataframe['tweet']]\n",
    "# non_offensive_train_tweets = [get_cleaned_tokens(tweet) for tweet in reg_train_dataframe['tweet']]\n",
    "all_train_tweets = offensive_train_tweets + non_offensive_train_tweets\n",
    "\n",
    "# Create Traianing Labels\n",
    "offensive_train_labels = off_train_dataframe['offensive']\n",
    "non_offensive_train_labels = reg_train_dataframe['offensive']\n",
    "all_train_labels = offensive_train_labels + non_offensive_train_labels\n",
    "\n",
    "\n",
    "# Create Test Tweets\n",
    "offensive_test_tweets = off_test_dataframe['tweet']\n",
    "non_offensive_test_tweets = reg_test_dataframe['tweet']\n",
    "# offensive_test_tweets = [get_cleaned_tokens(tweet) for tweet in off_test_dataframe['tweet']]\n",
    "# non_offensive_test_tweets = [get_cleaned_tokens(tweet) for tweet in reg_test_dataframe['tweet']]\n",
    "all_test_tweets = offensive_test_tweets + non_offensive_test_tweets\n",
    "\n",
    "# Create Test Labels\n",
    "offensive_test_labels = off_test_dataframe['offensive']\n",
    "non_offensive_test_labels = reg_test_dataframe['offensive']\n",
    "all_test_labels = offensive_test_labels + non_offensive_test_labels\n",
    "\n",
    "print(all_train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-f4d102e829c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msequences_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.1/envs/offensive-text-filter/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.7.1/envs/offensive-text-filter/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# tokenize words \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "VOCAB_SIZE = 100000\n",
    "tokenizer = Tokenizer(nb_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(all_train_tweets)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(all_train_tweets)\n",
    "sequences_test = tokenizer.texts_to_sequences(all_test_tweets)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "train_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(sequences_test, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(all_train_tweets))\n",
    "# print(len(all_test_tweets))\n",
    "# print(len(all_train_labels))\n",
    "# print(len(all_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "import os\n",
    "FILEPATH = \"./glove.twitter.27B.50d.txt\"\n",
    "f = open(os.path.join(FILEPATH))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 50)) #DIMENSIONS!\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 70, 50)            5000000   \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 3500)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                35010     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,035,021\n",
      "Trainable params: 35,021\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loss: 0.3887756029764811\n",
      "Accuracy: 83.333331\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAIN MODEL\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(VOCAB_SIZE, 50, weights=[embedding_matrix], input_length=70, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten()) # must use Flatter or GlobalMax\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(train_data, all_train_labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(test_data, all_test_labels, verbose=0)\n",
    "print(\"Loss: {}\".format(loss))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Conv1D(filter_num, kernel_size, activation = 'relu'))\n",
    "# model.add(keras.layers.GlobalMaxPooling1D())\n",
    "# model.add(keras.layers.Dense(10, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(offensive_vectors, non_offensive_vectors,\n",
    "#                     epochs=10,\n",
    "#                     verbose=False,\n",
    "#                     validation_data=(offensive_vectors_test, non_offensive_vectors_test),\n",
    "#                     batch_size=10)\n",
    "# loss, accuracy = model.evaluate(offensive_vectors, non_offensive_vectors, verbose=False)\n",
    "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# loss, accuracy = model.evaluate(offensive_vectors_test, non_offensive_vectors_test, verbose=False)\n",
    "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array\n",
    "# from numpy import asarray\n",
    "# from numpy import zeros\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Embedding\n",
    "# # define documents\n",
    "# docs = ['Well done!',\n",
    "# \t\t'Good work',\n",
    "# \t\t'Great effort',\n",
    "# \t\t'nice work',\n",
    "# \t\t'Excellent!',\n",
    "# \t\t'Weak',\n",
    "# \t\t'Poor effort!',\n",
    "# \t\t'not good',\n",
    "# \t\t'poor work',\n",
    "# \t\t'Could have done better.']\n",
    "# # define class labels\n",
    "# labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# # prepare tokenizer\n",
    "# t = Tokenizer()\n",
    "# t.fit_on_texts(docs)\n",
    "# vocab_size = len(t.word_index) + 1\n",
    "# # integer encode the documents\n",
    "# encoded_docs = t.texts_to_sequences(docs)\n",
    "# print(encoded_docs)\n",
    "# # pad documents to a max length of 4 words\n",
    "# max_length = 4\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# print(padded_docs)\n",
    "# # load the whole embedding into memory\n",
    "# embeddings_index = dict()\n",
    "# f = open('../glove_data/glove.6B/glove.6B.100d.txt')\n",
    "# for line in f:\n",
    "# \tvalues = line.split()\n",
    "# \tword = values[0]\n",
    "# \tcoefs = asarray(values[1:], dtype='float32')\n",
    "# \tembeddings_index[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
