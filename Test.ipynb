{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this prior to running test\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECOMPILED_WV_FILEPATH = \"./glove.twitter.27B.50d.txt\"\n",
    "TRAINING_OFFENSIVE_FILENAME = \"./training_offensive.csv\"\n",
    "TRAINING_REGULAR_FILENAME = \"./training_regular.csv\"\n",
    "TEST_OFFENSIVE_FILENAME = \"./test_offensive.csv\"\n",
    "TEST_REGULAR_FILENAME = \"./test_regular.csv\"\n",
    "\n",
    "#\n",
    "MAX_SEQUENCE_LENGTH = 70\n",
    "DIMENSIONS = 50\n",
    "\n",
    "# Model related variables\n",
    "FILTER_NUM = 32\n",
    "KERNEL_SIZE = 10\n",
    "VOCAB_SIZE = 100000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the four data sets \n",
    "# loop 0 to 35 and if in the sentence, replace it with the word's value from wordtovec\n",
    "# if no word then put in 0 instead\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "TRAINING_OFFENSIVE_FILENAME = \"./training_offensive.csv\"\n",
    "TRAINING_REGULAR_FILENAME = \"./training_regular.csv\"\n",
    "TEST_OFFENSIVE_FILENAME = \"./test_offensive.csv\"\n",
    "TEST_REGULAR_FILENAME = \"./test_regular.csv\"\n",
    "\n",
    "off_train_dataframe = pandas.read_csv(TRAINING_OFFENSIVE_FILENAME)\n",
    "off_test_dataframe = pandas.read_csv(TEST_OFFENSIVE_FILENAME)\n",
    "reg_train_dataframe = pandas.read_csv(TRAINING_REGULAR_FILENAME)\n",
    "reg_test_dataframe = pandas.read_csv(TEST_REGULAR_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "non_alphabet_pattern = re.compile(\"[^a-zA-Z]\")\n",
    "#NULL_VECTOR = fb_model.wv[\"\"]\n",
    "\n",
    "def get_cleaned_tokens(sentence):    \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = [re.sub(non_alphabet_pattern, \"\", w) for w in filtered_sentence]\n",
    "    filtered_sentence = [w for w in filtered_sentence if w != \"\"]\n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tweets = off_train_dataframe['tweet']\n",
    "all_train_tweets = all_train_tweets.append(reg_train_dataframe['tweet'])\n",
    "all_train_tweets = [get_cleaned_tokens(tweet) for tweet in all_train_tweets]\n",
    "all_train_labels = off_train_dataframe[\"offensive\"]\n",
    "all_train_labels = all_train_labels.append(reg_train_dataframe[\"offensive\"])\n",
    "\n",
    "all_test_tweets = off_test_dataframe['tweet']\n",
    "all_test_tweets = all_test_tweets.append(reg_test_dataframe['tweet'])\n",
    "all_test_tweets = [get_cleaned_tokens(tweet) for tweet in all_test_tweets]\n",
    "all_test_labels = off_test_dataframe[\"offensive\"]\n",
    "all_test_labels = all_test_labels.append(reg_test_dataframe[\"offensive\"])\n",
    "\n",
    "assert len(all_train_tweets) == len(all_train_labels), f\"{len(all_train_tweets)} != {len(all_train_labels)}\"\n",
    "assert len(all_test_tweets) == len(all_test_labels), f\"{len(all_test_tweets)} != {len(all_test_labels)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33731 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# tokenize words \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(100000)\n",
    "tokenizer.fit_on_texts(all_train_tweets)\n",
    "sequences_train = tokenizer.texts_to_sequences(all_train_tweets)\n",
    "sequences_test = tokenizer.texts_to_sequences(all_test_tweets)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "train_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "import os\n",
    "f = open(os.path.join(PRECOMPILED_WV_FILEPATH))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 50)) #DIMENSIONS!\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/var/pyenv/versions/3.7.1/envs/offensive-text-filter/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 70, 50)            5000000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 61, 32)            16032     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1952)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                19530     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,035,573\n",
      "Trainable params: 35,573\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/var/pyenv/versions/3.7.1/envs/offensive-text-filter/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ccd8470>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAIN MODEL\n",
    "\"\"\"\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(VOCAB_SIZE, 50, weights=[embedding_matrix], \n",
    "              input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Conv1D(FILTER_NUM, KERNEL_SIZE, activation = 'relu'))\n",
    "model.add(Flatten()) # must use Flatter or GlobalMax\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(train_data, all_train_labels, epochs=50, verbose=0)\n",
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4772871879800131\n",
      "Accuracy: 93.704599\n",
      "Loss: 0.004486987130572844\n",
      "Accuracy: 99.838603\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data, all_test_labels, verbose=0)\n",
    "print(\"Loss: {}\".format(loss))\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "loss, accuracy = model.evaluate(train_data, all_train_labels, verbose=0)\n",
    "print(\"Loss: {}\".format(loss))\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_sentence(tokenizer, sentence):\n",
    "    cleaned_words = get_cleaned_tokens(sentence)\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([cleaned_words])\n",
    "    return pad_sequences(tokenized_sentence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_sentence = \"\"\"termite eats bullets\"\"\"\n",
    "vectorized = vectorize_sentence(tokenizer, test_sentence)\n",
    "model.predict_classes(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   718,     8,   899],\n",
       "       [    0,     0,     0, ...,    33,    13,   319],\n",
       "       [    0,     0,     0, ...,   598,  4190, 24273],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    35,   516,  1613],\n",
       "       [    0,     0,     0, ...,    19,   499,   278],\n",
       "       [    0,     0,     0, ...,    17,    12,    15]], dtype=int32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anything with yo seems to be heavily weighted towards offensive\n",
    "# Garbage in, garbage out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(keras.layers.Conv1D(filter_num, kernel_size, activation = 'relu'))\n",
    "# model.add(keras.layers.GlobalMaxPooling1D())\n",
    "# model.add(keras.layers.Dense(10, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(offensive_vectors, non_offensive_vectors,\n",
    "#                     epochs=10,\n",
    "#                     verbose=False,\n",
    "#                     validation_data=(offensive_vectors_test, non_offensive_vectors_test),\n",
    "#                     batch_size=10)\n",
    "# loss, accuracy = model.evaluate(offensive_vectors, non_offensive_vectors, verbose=False)\n",
    "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# loss, accuracy = model.evaluate(offensive_vectors_test, non_offensive_vectors_test, verbose=False)\n",
    "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import array\n",
    "# from numpy import asarray\n",
    "# from numpy import zeros\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Embedding\n",
    "# # define documents\n",
    "# docs = ['Well done!',\n",
    "# \t\t'Good work',\n",
    "# \t\t'Great effort',\n",
    "# \t\t'nice work',\n",
    "# \t\t'Excellent!',\n",
    "# \t\t'Weak',\n",
    "# \t\t'Poor effort!',\n",
    "# \t\t'not good',\n",
    "# \t\t'poor work',\n",
    "# \t\t'Could have done better.']\n",
    "# # define class labels\n",
    "# labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# # prepare tokenizer\n",
    "# t = Tokenizer()\n",
    "# t.fit_on_texts(docs)\n",
    "# vocab_size = len(t.word_index) + 1\n",
    "# # integer encode the documents\n",
    "# encoded_docs = t.texts_to_sequences(docs)\n",
    "# print(encoded_docs)\n",
    "# # pad documents to a max length of 4 words\n",
    "# max_length = 4\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# print(padded_docs)\n",
    "# # load the whole embedding into memory\n",
    "# embeddings_index = dict()\n",
    "# f = open('../glove_data/glove.6B/glove.6B.100d.txt')\n",
    "# for line in f:\n",
    "# \tvalues = line.split()\n",
    "# \tword = values[0]\n",
    "# \tcoefs = asarray(values[1:], dtype='float32')\n",
    "# \tembeddings_index[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
